{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de55a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('./..')\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import concurrent.futures\n",
    "import pickle\n",
    "\n",
    "import Methods.common as common\n",
    "import Methods.dataLoader as dataloader\n",
    "\n",
    "try:\n",
    "    import argparse\n",
    "    import magic\n",
    "except ImportError as err:\n",
    "    print (err)\n",
    "    sys.exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231cea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = 'sorted_all_dataset.pkl'\n",
    "token_file = 'tokens.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a7f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_file, \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "token_list = list()\n",
    "with open(token_file) as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines[0].split(','):\n",
    "        token_list.append(line)\n",
    "        \n",
    "lenTokens = len(token_list)\n",
    "\n",
    "try:\n",
    "    common.magic_cookie = magic.open(magic.MAGIC_MIME)\n",
    "    common.magic_cookie.load()\n",
    "except AttributeError:\n",
    "    common.magic_cookie = magic.Magic(mime=True, uncompress=True)\n",
    "common.verbose_print('[-] initialized magic cookie\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5997f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getDataUsingThreads(params):\n",
    "    source, destination, prs, cut_off_date, task, begin, end = params\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    ct = 0\n",
    "    \n",
    "    working_repos = []\n",
    "    for pair in range(begin, end):\n",
    "        print(pair, '-', source[pair], '-', destination[pair])\n",
    "        common.verbose_mode = False\n",
    "\n",
    "        try:\n",
    "            destination_sha, ct = dataloader.getDestinationSha(destination[pair], cut_off_date[pair], token_list, ct)\n",
    "            ct, pr_data, req, runtime = dataloader.fetchPrData(source[pair], destination[pair], prs[pair], destination_sha, token_list, ct)\n",
    "            with open('Repos_prs/' + str(pair) + '_' + source[pair].split('/')[0] + '_' + source[pair].split('/')[1] + '_prs.pkl', 'wb') as f:\n",
    "                pickle.dump([pr_data, req, runtime], f)\n",
    "            working_repos.append(pair)\n",
    "        except Exception as e:\n",
    "            print('Exception  :(\\n ',e)\n",
    "    print(working_repos)        \n",
    "    finish = time.perf_counter()\n",
    "    return f'Task {task} done executing in ... {round((finish - start) / 60, 2)} minutes'\n",
    "\n",
    "def mainThread():\n",
    "    source = []\n",
    "    destination = []\n",
    "    prs = []\n",
    "    cut_off_date = []\n",
    "    \n",
    "    for i in dataset:\n",
    "        source.append(dataset[i]['source'])\n",
    "        destination.append(dataset[i]['destination'])\n",
    "        prs.append(str(dataset[i]['pr']).split('/'))\n",
    "        cut_off_date.append(dataset[i]['cut_off_date'])\n",
    "        \n",
    "    task = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    \n",
    "    start1 = 1\n",
    "    end1 = 2\n",
    "    start2 = 4\n",
    "    end2 = 6\n",
    "    start3 = 4\n",
    "    end3 = 6\n",
    "    start4 = 7\n",
    "    end4 = 10\n",
    "    start5 = 10\n",
    "    end5 = 15\n",
    "    start6 = 18\n",
    "    end6 = 19\n",
    "    start7 = 21\n",
    "    end7 = 31\n",
    "    start8 = 33\n",
    "    end8 = 45\n",
    "    start9 = 48\n",
    "    end9 = 50\n",
    "    start10 = 150\n",
    "    end10 = 342\n",
    "#     start10 = 301\n",
    "#     end10 = 321\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        params = [\n",
    "                  (source, destination, prs, cut_off_date, task[0], start1, end1),\n",
    "                  (source, destination, prs, cut_off_date, task[1], start2, end2),\n",
    "                  (source, destination, prs, cut_off_date, task[2], start3, end3),\n",
    "                  (source, destination, prs, cut_off_date, task[3], start4, end4),\n",
    "                  (source, destination, prs, cut_off_date, task[4], start5, end5),\n",
    "                  (source, destination, prs, cut_off_date, task[5], start6, end6),\n",
    "                  (source, destination, prs, cut_off_date, task[6], start7, end7),\n",
    "                  (source, destination, prs, cut_off_date, task[7], start8, end8),\n",
    "                  (source, destination, prs, cut_off_date, task[8], start9, end9),\n",
    "                  (source, destination, prs, cut_off_date, task[9], start10, end10)\n",
    "                 ]\n",
    "        results = executor.map(getDataUsingThreads, params)\n",
    "\n",
    "        for result in results:\n",
    "            print(result)\n",
    "\n",
    "mainThread()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
